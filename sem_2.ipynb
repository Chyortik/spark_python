{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Семинар 2"
      ],
      "metadata": {
        "id": "nJEnz3GX2HhT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyspark >> None"
      ],
      "metadata": {
        "id": "kXpTOqvn2Nww"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O2GVxdk02BET"
      },
      "outputs": [],
      "source": [
        "import pyspark\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import sum, avg, when, max, month, year, countDistinct\n",
        "from pyspark.sql import functions as F\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "spark=SparkSession.builder.appName('Practise').getOrCreate()\n",
        "\n",
        "df_pyspark = spark.read.load('sales_data.csv', format='csv', sep=',', header='true', Infer_schema=True)\n",
        "\n",
        "df_pyspark = df_pyspark.withColumn('order_id', df_pyspark['order_id'].cast('int'))\n",
        "df_pyspark = df_pyspark.withColumn('product_id', df_pyspark['product_id'].cast('int'))\n",
        "df_pyspark = df_pyspark.withColumn('customer_id', df_pyspark['customer_id'].cast('int'))\n",
        "df_pyspark = df_pyspark.withColumn('order_date', df_pyspark['order_date'].cast('date'))\n",
        "df_pyspark = df_pyspark.withColumn('quantity', df_pyspark['quantity'].cast('int'))\n",
        "df_pyspark = df_pyspark.withColumn('price_per_unit', df_pyspark['price_per_unit'].cast('int'))\n",
        "df_pyspark = df_pyspark.withColumn('total_price', df_pyspark['total_price'].cast('int'))\n",
        "\n",
        "df_pyspark = df_pyspark.withColumn('order_month', month(df_pyspark['order_date']))\n",
        "\n",
        "sales_analysis = df_pyspark.groupBy('order_month').agg(\n",
        "                                    sum('total_price').alias('total_sales'),\n",
        "                                    avg('total_price').alias('average_sales'),\n",
        "                                    max('total_price').alias('max_sales')\n",
        "                                    )\n",
        "\n",
        "sales_analysis.orderBy('order_month').show()\n"
      ],
      "metadata": {
        "id": "XAjUkYyl2VrJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0CiHfh8rfiH5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Задание 2. Вычислите количество товаров, купленных различными методами оплаты"
      ],
      "metadata": {
        "id": "7OTrzIeI50nY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sales_analysis_2 = df_pyspark.groupBy('payment_method').agg(sum('quantity').alias('num_of_sales'))\n",
        "sales_analysis_2.show()"
      ],
      "metadata": {
        "id": "aNCfxgug2Vtz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Задание 3. Найдите регион с самой большой суммарной стоимостью продаж"
      ],
      "metadata": {
        "id": "7httRQds6cMV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sales_analysis_3 = df_pyspark.groupBy('region').agg(sum('total_price').alias('total_price_per_region'))\n",
        "total_price_per_region = sales_analysis_3.select('region').orderBy(sales_analysis_3.total_price_per_region.desc()).first()\n",
        "print(*total_price_per_region)"
      ],
      "metadata": {
        "id": "UVRW4Sg22VwE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Задание 4. Вычислите общую сумму продаж и среднюю сумму продажи для каждого региона"
      ],
      "metadata": {
        "id": "whxRHJRH2G2z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sales_analysis_4 = df_pyspark.groupBy('region').agg(sum('total_price').alias('total_price_per_region'), avg('total_price').alias('total_price_per_region'))\n",
        "sales_analysis_4.show()"
      ],
      "metadata": {
        "id": "MGUCVuBa2CaD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Задание 5. Вычислите общее количество и сумму товаров, проданных за наличные в 2022 году"
      ],
      "metadata": {
        "id": "36AD6VlT7sWn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_pyspark = df_pyspark.withColumn('order_year', year(df_pyspark['order_date']))\n",
        "\n",
        "sales_analysis_5 = df_pyspark.filter((df_pyspark.payment_method == 'Наличные') & (df_pyspark.order_year == 2022)).agg(\n",
        "                                      sum('quantity').alias('total_quantity_2022'), sum('total_price').alias('total_price_2022')\n",
        "                                      )\n",
        "sales_analysis_5.show()"
      ],
      "metadata": {
        "id": "Rpf7NBel7tYC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Задание 6. Найдите уникальное количество покупателей за 2022 год"
      ],
      "metadata": {
        "id": "Wyglf0l-8wXI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sales_analysis_6 = df_pyspark.filter(df_pyspark.order_year == 2022)\n",
        "sales_analysis_6 = sales_analysis_6.select(countDistinct('customer_id').alias('unique_users'))\n",
        "\n",
        "sales_analysis_6.show()"
      ],
      "metadata": {
        "id": "2OKM3pnZ2Cch"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Задание 7. Вам даны данные с информацией о стоимости продуктов в различных валютах. Ваша задача состоит в том, чобы перевести все цены в доллары, используя текущие курсы валют. Однако, у вас есть ограничение: для некоторых продуктов курс валюты неизвестен и их стоимость должна остаться в исходной валюте. (Для конвертации из EUR в USD нужно умножить на 1.2)"
      ],
      "metadata": {
        "id": "3urlwKoa9gTs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "spark = SparkSession.builder.appName('currency_conversion').getOrCreate()\n",
        "\n",
        "data = [(1, 100, 'USD'), (2, 200, 'EUR'), (3, 300, 'Unknown'),\n",
        "        (4, 100, 'EUR'), (5, 200, 'EUR'), (6, 300, 'Unknown'),\n",
        "        (7, 100, 'Unknown'), (8, 200, 'USD'), (9, 300, 'USD')]\n",
        "\n",
        "df_7 = spark.createDataFrame(data, ['product_id', 'price', 'currency'])\n",
        "\n",
        "df_7 = df_7.withColumn('price_usd', when(df_7.currency == 'USD', df_7.price)\n",
        "                                .when(df_7.currency == 'USD', df_7.price * 1.2)\n",
        "                                .otherwise(df_7.price))\n",
        "df_7.show()"
      ],
      "metadata": {
        "id": "g5fvT_RK2CfE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Задание 8. Допустим, есть два датасета: один содержит информацию о пользователях(user_id, name, age), а другой содержит информацию о покупках пользователей (user_id, product_id, date). Необходимо найти  средний возраст пользователей, совершивших покупки"
      ],
      "metadata": {
        "id": "11sdQYUaAIpe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "spark = SparkSession.builder.appName('user_purchase_join').getOrCreate()\n",
        "\n",
        "users_df = spark.createDataFrame(\n",
        "          [(1, 'Alice', 25), (2, 'Bob', 30), (3, 'Charlie', 28),\n",
        "            (4, 'John', 56), (5, 'Alex', 41), (6, 'Juliya', 17)], ['user_id', 'name', 'age']\n",
        "          )\n",
        "\n",
        "purchases_df = spark.createDataFrame(\n",
        "          [(1, 101, '2022-01-01'), (2, 102, '2022-01-02'), (3, 103, '2022-01-03'),\n",
        "           (3, 104, '2022-01-04'), (6, 105, '2022-01-05')], ['user_id', 'product_id', 'date']\n",
        "          )\n",
        "\n",
        "result_df = users_df.join(purchases_df, 'users.id').groupBy('user_id').agg(avg('age').alias('average_age'))\n",
        "\n",
        "result_df.show()"
      ],
      "metadata": {
        "id": "-nFz48TP2Cha"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Задание 9. У вас есть два набора данных. Первый набор содержит информацию о продуктах: id продукта, название, категория и цена. Второй набор содержит информацию о заказах: id заказа, id продукта, количество. Ваша задача - использовать PySpark для выполнения следующих шагов:\n",
        "1. Присоединить набор данных о продуктах к набору данных о заказах с помощью id продукта\n",
        "2. Рассчитать общую стоимость каждого заказа, учитывая количество продуктов и их цену.\n",
        "3. Отфильтровать заказы, у которых стоимость больше 1000"
      ],
      "metadata": {
        "id": "Rao5EtonC1gg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "spark = SparkSession.builder.appName('aggregate_join_filter').getOrCreate()\n",
        "\n",
        "data_products = [(1, 'product1', 'category1', 10.0), (2, 'product2', 'category2', 15.0), (3, 'product3', 'category1', 12.0),\n",
        "                (4, 'product4', 'category3', 20.0), (5, 'product5', 'category2', 18.0), (6, 'product6', 'category3', 25.0),\n",
        "                (7, 'product7', 'category1', 9.0), (8, 'product8', 'category2', 16.0), (9, 'product9', 'category3', 22.0),\n",
        "                (10, 'product10', 'category1', 11.5)]\n",
        "\n",
        "products_df = spark.createDataFrame(data_products, ['product_id', 'title', 'category', 'price'])\n",
        "\n",
        "data_orders = [(1, 1, 5), (2, 3, 2), (3, 2, 3), (4, 5, 1), (5, 4, 4),\n",
        "               (6, 7, 2), (7, 6, 3), (8, 8, 2), (9, 10, 1), (10, 9, 4)]\n",
        "\n",
        "orders_df = spark.createDataFrame(data_orders, ['order_id', 'product_id', 'quantity'])\n",
        "\n",
        "joined_df = orders_df.join(products_df, 'product_id')\n",
        "\n",
        "total_cost_df = joined_df.withColumn('total_cost', F.col('quantity') * F.col('price'))\n",
        "\n",
        "filtered_orders_df = total_cost_df.filter(total_cost_df.total_cost > 1000)\n",
        "\n",
        "filtered_orders_df.show()"
      ],
      "metadata": {
        "id": "snM1fXTR2Cjx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Задание 10. Найти сумму чисел в колонке"
      ],
      "metadata": {
        "id": "WUtq0PLPG2q-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "spark = SparkSession.builder.appName('sum_example').getOrCreate()\n",
        "\n",
        "data_10 = [(1,), (2,), (3,), (4,), (5,)]\n",
        "df_10 = spark.createDataFrame(data_10, ['number'])\n",
        "\n",
        "sum_result = df_10.select(sum('number'))\n",
        "\n",
        "sum_result.show()"
      ],
      "metadata": {
        "id": "XRLsrKgh2Cov"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Задание 11. Посчитать количество уникальных значений в колонке"
      ],
      "metadata": {
        "id": "n91tfej4Hsyz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "spark = SparkSession.builder.appName('count_distinct_example').getOrCreate()\n",
        "\n",
        "data_11 = [('Alice',), ('Bob',), ('Alice',), ('Eve',)]\n",
        "\n",
        "df_11 = spark.createDataFrame(data_11, ['name'])\n",
        "count_result = df_11.select(countDistinct('name'))\n",
        "\n",
        "count_result.show()"
      ],
      "metadata": {
        "id": "-qBMam8K2CrD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Задание 12. Выполнить фильтрацию данных по определенному условию."
      ],
      "metadata": {
        "id": "b9FMruKfIfEb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "spark = SparkSession.builder.appName('filter_example').getOrCreate()\n",
        "\n",
        "data_12 = [('Alice', 25), ('Bob',30 ), ('Eve', 20), ('Charlie', 35)]\n",
        "df_12 = spark.createDataFrame(data_12, ['name', 'age'])\n",
        "\n",
        "filtered_data = df_12.filter(df_12.age < 30)\n",
        "\n",
        "filtered_data.show()"
      ],
      "metadata": {
        "id": "DnQl4Zw52Ctu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Домашнее задание. Условие: дана таблица с колонками (id, name, salary, manager_id). Студентам необходимо написать код на spark, который создаст эту таблицу (данные указаны ниже) и в результате выдаст таблицу, в которой будут имена сотрудников, которые зарабатывают больше своих менеджеров"
      ],
      "metadata": {
        "id": "1tRmz0zpJGFL"
      }
    }
  ]
}