{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Урок 3. DataFrames"
      ],
      "metadata": {
        "id": "9GSkpTm2m6dp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Как создать DataFrame\n",
        "Есть несколько способов для создания DataFrame.\n",
        "* из python list\n",
        "* прочитать из файла\n",
        "* прочитать таблицу из Hive\n",
        "* прочитать таблицу по jdbc\n",
        "* range\n"
      ],
      "metadata": {
        "id": "W09cuDA5nKV0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyspark"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WHW4Pwc8ne-a",
        "outputId": "c2d9043c-f459-47b4-8554-2d825a58ce29"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyspark\n",
            "  Downloading pyspark-3.5.1.tar.gz (317.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.0/317.0 MB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n",
            "Building wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.5.1-py2.py3-none-any.whl size=317488491 sha256=fb7a32f9ef407c644ac2b5bbcdf0dfd43a2a7c46d71acdadfca0825de3e0b5c3\n",
            "  Stored in directory: /root/.cache/pip/wheels/80/1d/60/2c256ed38dddce2fdd93be545214a63e02fbd8d74fb0b7f3a6\n",
            "Successfully built pyspark\n",
            "Installing collected packages: pyspark\n",
            "Successfully installed pyspark-3.5.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### из python list"
      ],
      "metadata": {
        "id": "lERH1Wlkpg2W"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "QFIaseWWms67",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d499e2e5-d734-4eea-94e5-e781cd1f354b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+---+\n",
            "| name|age|\n",
            "+-----+---+\n",
            "| Ivan| 10|\n",
            "| Petr| 20|\n",
            "|Elena| 30|\n",
            "+-----+---+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Здесь мы инициализируем spark сессию\n",
        "from pyspark.sql import SparkSession\n",
        "spark = (SparkSession.builder.master('local[*]').getOrCreate())\n",
        "data = [('Ivan', 10),\n",
        "        ('Petr', 20),\n",
        "        ('Elena', 30)]\n",
        "\n",
        "columns = ['name', 'age']\n",
        "\n",
        "df = spark.createDataFrame(data=data, schema=columns)\n",
        "df.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### прочитать из файла"
      ],
      "metadata": {
        "id": "km-yZLpCpjmo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "spark = (SparkSession.builder.master('local[*]').getOrCreate())\n",
        "\n",
        "df = spark.read.option('header', 'true').csv('data_3.csv')\n",
        "df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w4HA6wg-m4ZK",
        "outputId": "a2e88fc1-f710-4a74-88dc-1140ba36a00d"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+---+\n",
            "| name|age|\n",
            "+-----+---+\n",
            "| Ivan| 10|\n",
            "| Petr| 20|\n",
            "|Elena| 30|\n",
            "+-----+---+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### прочитать таблицу из Hive"
      ],
      "metadata": {
        "id": "DQjMVSXiq0_C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "В крупных компаниях часто используют СУБД\n",
        "Apache Hive, коннект с которой доступен в spark\n",
        "сразу.\n",
        "Инициализируем спарк сессию, но с настройкой\n",
        "enableHiveSupport(). Эта настройка позволяет нам\n",
        "считывать и записывать таблицы, используя только\n",
        "название таблицы, т.е вам не нужно прописывать\n",
        "путь хранения данных.\n",
        "В 5 строке мы используем метод table(), параметр\n",
        "которого это название таблицы. Этот метод вернет\n",
        "нам DataFrame с данными из таблицы Hive"
      ],
      "metadata": {
        "id": "WsKpAIPMq6Rx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# from pyspark.sql import SparkSession\n",
        "# spark = (SparkSession.builder.enableHiveSupport().getOrCreate())\n",
        "\n",
        "# df = spark.table('table_name')\n",
        "# df.show()"
      ],
      "metadata": {
        "id": "x5KtlKUcm4bw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### прочитать таблицу по jdbc"
      ],
      "metadata": {
        "id": "DqE12IhsrKRi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "JDBC (Java Database Connector) - специальная\n",
        "библиотека, написанная на java, позволяющая\n",
        "подключаться к базе данных (Postgres, Oracle,\n",
        "GreenPlum и тд) и читать или записывать туда\n",
        "данные.\n",
        "Здесь мы инициализируем спарк сессию, в\n",
        "переменной creds у нас словарь, в котором указаны\n",
        "логин и пароль для подключения к базе данных\n",
        "Затем мы читаем данные с помощью метода jdbc, в\n",
        "нем указываем url базы данных, нужную нам\n",
        "таблицу (которую мы хотим прочитать), properties -\n",
        "это наш словарь creds, в котором указаны наши\n",
        "логин и пароль."
      ],
      "metadata": {
        "id": "pijDLwVMrN-j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# from pyspark.sql import SparkSession\n",
        "# spark = (SparkSession.builder.master('local[*]').getOrCreate())\n",
        "\n",
        "# creds = {'user': 'name', 'password': '12345'}\n",
        "# df = spark.read.jdbc('url', 'table', properties=creds)"
      ],
      "metadata": {
        "id": "wyEyeRNQm4et"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### range"
      ],
      "metadata": {
        "id": "jXNvBPn6roTV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Самый простой способ создать датафрейм, в\n",
        "котором будут числа, а столбец будет называться id\n",
        "инициализируем спарк сессию, затем используем\n",
        "метод range и указываем количество чисел в\n",
        "датафрейме"
      ],
      "metadata": {
        "id": "KuDVMqD0rs-o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "spark = (SparkSession.builder.master('local[*]').getOrCreate())\n",
        "\n",
        "df = spark.range(100)\n",
        "df.show(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QRZaml-2m4hO",
        "outputId": "ca4c82c1-98ab-4d3e-a569-7313e80e6a65"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+\n",
            "| id|\n",
            "+---+\n",
            "|  0|\n",
            "|  1|\n",
            "|  2|\n",
            "|  3|\n",
            "|  4|\n",
            "+---+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Функции\n",
        "#### DataFrames в Apache Spark предоставляют множество функций для работы с данными. Некоторые из них:\n",
        "* select: выбор столбцов из датафрейма\n",
        "* filter: фильтрация строк датафрейма на основе условия\n",
        "* withColumn: создание столбца в датафрейме\n",
        "* withColumnRenamed: переименование столбца в датафрейме\n",
        "* groupBy: группировка строк датафрейма по значениям в столбцах\n",
        "* agg: агрегация данных в датафрейме с использованием агрегатных функций, таких как сумма, среднее\n",
        "и т.д.\n",
        "* join: объединение двух датафреймов на основе заданных условий\n",
        "* orderBy: сортировка строк датафрейма по значениям в столбцах\n",
        "\n",
        "Кроме того, в DataFrames есть множество других функций, таких как distinct, dropDuplicates, count,\n",
        "mean, sum, max, min и т.д., которые позволяют проводить анализ данных и получать информацию о данных."
      ],
      "metadata": {
        "id": "w5Zpjn1Ir-ZT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# select\n",
        "from pyspark.sql.functions import col\n",
        "df.select(col('id')) # вернет одну колонку id"
      ],
      "metadata": {
        "id": "PnsMi5N1m4kG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# filter\n",
        "df.filter(col('id') < 50) # вернет колонку id со значениями менее 50"
      ],
      "metadata": {
        "id": "Xs2R0DwDm4mq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# withColumn\n",
        "df.withColumn('new', col('id') % 2)\n",
        "# первый параметр - имя нового столбца\n",
        "# второй параметр - значения нового столбца. Здесь значения определяются таким образом,\n",
        "# то есть в столбце будут значение 0 и 1"
      ],
      "metadata": {
        "id": "OuSL5nwdm4pP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# withColumnRenamed\n",
        "df.withColumnRenamed('id', 'id_rename')"
      ],
      "metadata": {
        "id": "JbnJGxXem4rt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# groupby\n",
        "df.withColumn('new', col('id') % 2).groupby(col('new'))"
      ],
      "metadata": {
        "id": "VYMGyyoQm4uk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# agg\n",
        "(df.withColumn('new', col('id') % 2).groupby(col('new')).agg(max(col('id'))))\n",
        "# здесь мы делаем агрегацию - находим максимальное число в id при группировке по new"
      ],
      "metadata": {
        "id": "kbX2mGLMm4xC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Join\n",
        "df2= spark.range(10).withColumnRenamed('id', 'id2')\n",
        "df.join(df2,col('id') == col('id2'), 'inner')\n",
        "# соединяем оба датафрейма по столбцам id и id2\n",
        "# тип джойна inner, есть и другие"
      ],
      "metadata": {
        "id": "BIXBdQR6m4z0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# orderBy\n",
        "df.orderBy(col('id').desc) # по убыванию\n",
        "df.orderBy(col('id').asc) # по возрастанию"
      ],
      "metadata": {
        "id": "ku0ZaVA8m42g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# distinct\n",
        "df.distinct()\n",
        "# убираем дубли из датафрейма"
      ],
      "metadata": {
        "id": "z9pkKQsem45O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Группировки и агрегации"
      ],
      "metadata": {
        "id": "2LarCBr3vnij"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col, mean, collect_list\n",
        "spark = (SparkSession.builder.master('local[*]').getOrCreate())\n",
        "\n",
        "df = spark.read.option('header', 'true').csv('data_33.csv')\n",
        "\n",
        "df.groupBy(col('name')).agg(mean(col('age')).alias('count_age')).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vQLTEb8tm470",
        "outputId": "3e38a5b6-4d20-4ab1-847e-4ff6f2b5e91a"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+------------------+\n",
            "| name|         count_age|\n",
            "+-----+------------------+\n",
            "| Lena|              50.0|\n",
            "| Petr|              13.0|\n",
            "| Ivan|53.333333333333336|\n",
            "|Katya|              22.5|\n",
            "+-----+------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.groupBy(col('name')).agg(collect_list(col('age')).alias('count_age')).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LcC8f5Y6m4-i",
        "outputId": "51369c87-a6a4-4426-87da-207fccd93043"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+------------+\n",
            "| name|   count_age|\n",
            "+-----+------------+\n",
            "| Lena|[25, 80, 45]|\n",
            "| Petr|     [25, 1]|\n",
            "| Ivan|[50, 50, 60]|\n",
            "|Katya|    [10, 35]|\n",
            "+-----+------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Оконные функции\n",
        "Оконная функция — это функция, которая выполняет агрегирующую функцию на определенном наборе (окне,\n",
        "партиции) строк и результат записывает в новый столбец в таблице"
      ],
      "metadata": {
        "id": "jjOJsoSxxIRn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "к оконным функциям относятся такие функции, как count, mean, sum, max, min и т.д., а также ранжирующие функции row_number, rank, dense_rank, функции смещения lag (смещает все значения назад), lead (смещает все значения вперед)"
      ],
      "metadata": {
        "id": "9mRRaB18x9zd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Общие правила применения оконных функций\n",
        "# импортируем нужные функции\n",
        "from pyspark.sql.window import Window\n",
        "from pyspark.sql.functions import rank"
      ],
      "metadata": {
        "id": "l3t4p8Kam5BP"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.withColumn('rank_col', rank().over(Window.partitionBy('partition_col').orderBy('orderby_col')))"
      ],
      "metadata": {
        "id": "-IhhEw01m5D9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# When и otherwise\n",
        "from pyspark.sql.functions import when, col\n",
        "\n",
        "spark = (SparkSession.builder.getOrCreate())\n",
        "\n",
        "df = spark.range(100)\n",
        "\n",
        "df.withColumn('test_col', when(col('id') > 50, 'value_more_50').otherwise('value_less_50')).show(2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KigfJeASm5Gr",
        "outputId": "2206fef4-3827-4578-bbcf-8bc4d198db6a"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+-------------+\n",
            "| id|     test_col|\n",
            "+---+-------------+\n",
            "|  0|value_less_50|\n",
            "|  1|value_less_50|\n",
            "+---+-------------+\n",
            "only showing top 2 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Pivot\n",
        "Pivot - позволяет развернуть сгруппированные значения строк в столбцы\n"
      ],
      "metadata": {
        "id": "Buabcxn81RBN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col, mean, collect_list\n",
        "spark = (SparkSession.builder.master('local[*]').getOrCreate())\n",
        "\n",
        "# df = spark.read.option('header', 'true').csv('data_pivot.csv')\n",
        "\n",
        "\n",
        "data = [('Ivan', 'Moscow', 150000),\n",
        "        ('Lena', 'Moscow', 100000),\n",
        "        ('Ivan', 'Moscow', 20000),\n",
        "        ('Petr', 'Sochi', 60000),\n",
        "         ('Roma', 'Sochi', 50000),\n",
        "        ('Roma', 'Sochi', 10000)]\n",
        "\n",
        "\n",
        "df = spark.createDataFrame(data=data).toDF('name', 'city', 'salary')\n",
        "df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "219ALq9bm5Jb",
        "outputId": "0dbcb2fc-3857-4788-8b16-b9b4ea1cb0a9"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+------+------+\n",
            "|name|  city|salary|\n",
            "+----+------+------+\n",
            "|Ivan|Moscow|150000|\n",
            "|Lena|Moscow|100000|\n",
            "|Ivan|Moscow| 20000|\n",
            "|Petr| Sochi| 60000|\n",
            "|Roma| Sochi| 50000|\n",
            "|Roma| Sochi| 10000|\n",
            "+----+------+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# группировка по столбцам и нахождения средней зп по городам\n",
        "df.groupby(col('city'), col('name')).agg(mean(col('salary'))).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4KFRox_um5MI",
        "outputId": "b17a657a-0a28-4e0f-9bb6-537630164bbd"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+----+-----------+\n",
            "|  city|name|avg(salary)|\n",
            "+------+----+-----------+\n",
            "|Moscow|Ivan|    85000.0|\n",
            "|Moscow|Lena|   100000.0|\n",
            "| Sochi|Petr|    60000.0|\n",
            "| Sochi|Roma|    30000.0|\n",
            "+------+----+-----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# развернем данные\n",
        "df.groupby('city').pivot('name').agg(mean('salary')).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mhQ7Ehy4m5Ou",
        "outputId": "b0b5029b-d4f3-4243-bc1a-07e133339e5e"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+-------+--------+-------+-------+\n",
            "|  city|   Ivan|    Lena|   Petr|   Roma|\n",
            "+------+-------+--------+-------+-------+\n",
            "|Moscow|85000.0|100000.0|   NULL|   NULL|\n",
            "| Sochi|   NULL|    NULL|60000.0|30000.0|\n",
            "+------+-------+--------+-------+-------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Cast\n",
        "Функция Cast - меняет тип данных"
      ],
      "metadata": {
        "id": "DRYKSuzf80do"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = (SparkSession.builder.master('local[*]').getOrCreate())\n",
        "\n",
        "df = spark.range(100)\n",
        "df.printSchema()\n",
        "# длинное число"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sJiNX6O0m5RN",
        "outputId": "823198f7-f3bd-4cd0-f94c-2c1ff7638606"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- id: long (nullable = false)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "new_df = df.select(col('id').cast('string'))\n",
        "new_df.printSchema()\n",
        "\n",
        "# или\n",
        "\n",
        "# from pyspark.sql.types import StringType\n",
        "# new_df2 = df.select(col('id').cast(StringType))\n",
        "# длинное число заменено на строкое значение"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d-NV7grkm5UK",
        "outputId": "b1431102-7555-408a-cc2f-e29c6b245422"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- id: string (nullable = false)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# другой способ\n",
        "new_df2 = df.withColumn('id', col('id').cast('string'))\n",
        "new_df2.printSchema()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JLpNfBcfm5W5",
        "outputId": "e3b90839-4662-4aa4-8106-19e2bda4f4b8"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- id: string (nullable = false)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### UDF\n",
        "UDF - user defined function - это функции, определяемые пользователем.\n",
        "Эти функции используются, когда вы реализовывайте более сложную логику."
      ],
      "metadata": {
        "id": "ZxOGZ02e-_-k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import udf\n",
        "spark = (SparkSession.builder.master('local[*]').getOrCreate())\n",
        "\n",
        "# df = spark.read.option('header', 'true').csv('data_pivot.csv')\n",
        "\n",
        "\n",
        "data = [('Iphone', 'Iphone14', 150000),\n",
        "        ('Huawei', 'Mate14', 100000),\n",
        "        ('Samsung', 'A14', 20000),\n",
        "        ('Iphone', 'Iphone13', 60000),\n",
        "         ('Huawei', 'Mate12', 50000),\n",
        "        ('Samsung', 'A11', 5000)]\n",
        "\n",
        "\n",
        "df_phones = spark.createDataFrame(data=data).toDF('company', 'model', 'price')\n",
        "df_phones.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gy0OfOlhm5ZX",
        "outputId": "a1195a41-0102-4a30-c4c0-feed4f0e410a"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+--------+------+\n",
            "|company|   model| price|\n",
            "+-------+--------+------+\n",
            "| Iphone|Iphone14|150000|\n",
            "| Huawei|  Mate14|100000|\n",
            "|Samsung|     A14| 20000|\n",
            "| Iphone|Iphone13| 60000|\n",
            "| Huawei|  Mate12| 50000|\n",
            "|Samsung|     A11|  5000|\n",
            "+-------+--------+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def recognize_model(model: str) -> str:\n",
        "    if '14' in model:\n",
        "        return 'new model'\n",
        "    else:\n",
        "        return 'old model'\n",
        "\n",
        "recognize_model_udf = udf(lambda x: recognize_model(x))\n",
        "\n",
        "new_df = df_phones.withColumn('version_model', recognize_model_udf('model')).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VGGDibQ0m5cF",
        "outputId": "64f7d27e-f8a0-4f30-bf64-3bab2c881728"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+--------+------+-------------+\n",
            "|company|   model| price|version_model|\n",
            "+-------+--------+------+-------------+\n",
            "| Iphone|Iphone14|150000|    new model|\n",
            "| Huawei|  Mate14|100000|    new model|\n",
            "|Samsung|     A14| 20000|    new model|\n",
            "| Iphone|Iphone13| 60000|    old model|\n",
            "| Huawei|  Mate12| 50000|    old model|\n",
            "|Samsung|     A11|  5000|    old model|\n",
            "+-------+--------+------+-------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Сохранение данных\n",
        "Для сохранения данных из DataFrames в Apache Spark можно использовать метод write объекта DataFrame,\n",
        "который позволяет записывать данные в различные источники, такие как файлы, базы данных, S3 и другие.\n",
        "Например, чтобы сохранить данные в CSV файл, можно использовать следующий код:\n",
        "df.write.csv(\"path/to/output/folder\", header=True)\n",
        "В данном примере мы записываем данные в CSV файл, указывая путь к выходной папке и опцию\n",
        "header=True, которая указывает на запись заголовка в файл."
      ],
      "metadata": {
        "id": "rxEFa3u-AzmG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_phones.write.csv(\"phones.csv\", header=True)"
      ],
      "metadata": {
        "id": "c1ZfEVxCm5ec"
      },
      "execution_count": 56,
      "outputs": []
    }
  ]
}